{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba42e07-87c6-44e7-a6da-9a8a927b736d",
   "metadata": {},
   "source": [
    "## Importemos las cosas importantes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903c62bb-1113-4c8a-9622-70a94bf4dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87f337f-24a8-41a1-a64a-a5b19ef1afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carguemos los datos!\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa9eb0-92f1-438f-b0ab-fabb3f2bff35",
   "metadata": {},
   "source": [
    "¡Ahora, saquemos de los datos que no nos importa (¡pero dejamos el identificador!) y lo filtraremos por año también porque crearemos un clasificador de los años 80!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f25e19c-3fe9-436d-8408-0912b13846fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_atributos = [\"name\", \"artists\", \"release_date\"]\n",
    "df.drop(drop_atributos, inplace=True, axis=1)\n",
    "df = df[(1980<=df.year) & (df.year<=1989)]\n",
    "y = df[\"year\"]; # nuestra variable a clasificar\n",
    "X = df[[name for name in df.columns if name not in [\"year\",\"id\"]]] # el conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b3661-52e5-49cf-ad26-bf7930fa7e16",
   "metadata": {},
   "source": [
    "Ahora vamos a entrenar a nuestro clasificador con el `DecisionTreeClassifier` con todos los atributos y veremos que sale por el momento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e11083-3d72-49b6-8e59-8280015d418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en test set: 0.1435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1980       0.15      0.15      0.15       600\n",
      "        1981       0.13      0.13      0.13       600\n",
      "        1982       0.16      0.16      0.16       600\n",
      "        1983       0.14      0.14      0.14       600\n",
      "        1984       0.12      0.13      0.13       600\n",
      "        1985       0.14      0.13      0.14       600\n",
      "        1986       0.16      0.16      0.16       600\n",
      "        1987       0.16      0.16      0.16       600\n",
      "        1988       0.16      0.16      0.16       600\n",
      "        1989       0.11      0.11      0.11       600\n",
      "\n",
      "    accuracy                           0.14      6000\n",
      "   macro avg       0.14      0.14      0.14      6000\n",
      "weighted avg       0.14      0.14      0.14      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ahora vamos a entrenar a nuestro clasificador con todos los atributos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=37, stratify=y)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=2)\n",
    "clf.fit(X_train, y_train)    ## Entrenamos con features X_train y clases y_train\n",
    "\n",
    "y_pred = clf.predict(X_test)   ## Predecimos con nuevos datos (los de test X_test)\n",
    "\n",
    "print(\"Accuracy en test set:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))## Evaluamos la predicción comparando y_test con y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242278c6-e72a-4261-969e-53c8425542f7",
   "metadata": {},
   "source": [
    "Podemos ver que no da un resultado tan bueno, obteniendo que los avg son de 0.14 para todas las metricas\n",
    "\n",
    "Ahora, sabemos de la exploracion que `energy`, `acousticness` y `popularity` tienen una correlacion con los años, entonces veamos que sucede si solo entrenamos al clasificador con estos parametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5406ec23-c06f-45a3-a1ba-069ed065c30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en test set: 0.115375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1980       0.13      0.12      0.13       800\n",
      "        1981       0.10      0.11      0.11       800\n",
      "        1982       0.12      0.13      0.13       800\n",
      "        1983       0.12      0.12      0.12       800\n",
      "        1984       0.10      0.10      0.10       800\n",
      "        1985       0.11      0.12      0.11       800\n",
      "        1986       0.14      0.12      0.13       800\n",
      "        1987       0.11      0.12      0.11       800\n",
      "        1988       0.12      0.12      0.12       800\n",
      "        1989       0.10      0.10      0.10       800\n",
      "\n",
      "    accuracy                           0.12      8000\n",
      "   macro avg       0.12      0.12      0.12      8000\n",
      "weighted avg       0.12      0.12      0.12      8000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Temp\\ipykernel_20196\\1103907536.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[\"popularity\"]=f(X[\"popularity\"])\n"
     ]
    }
   ],
   "source": [
    "#metodo para normalizar\n",
    "def f(x):\n",
    "    return x/(max(x)-min(x))\n",
    "X[\"popularity\"]=f(X[\"popularity\"])\n",
    "\n",
    "X_new = X[[\"energy\",\"acousticness\",\"popularity\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=.40, random_state=37, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=2)\n",
    "clf.fit(X_train, y_train)    ## Entrenamos con features X_train y clases y_train\n",
    "\n",
    "y_pred = clf.predict(X_test)   ## Predecimos con nuevos datos (los de test X_test)\n",
    "\n",
    "print(\"Accuracy en test set:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred)) ## Evaluamos la predicción comparando y_test con y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05709912-fc5c-4166-a85c-3167cacc0a9e",
   "metadata": {},
   "source": [
    "Podemos ver que en general la clasificación no muestra buenos resultados, por lo que veremos si es posible obtener mejores resultados a partir de clasificar, con columnas que tengan mayor correlación.\n",
    "\n",
    "En particular con las propiedades de acousticness y popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f340af8-524b-41cf-863c-7945b9572aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en test set: 0.112125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1980       0.11      0.12      0.12       800\n",
      "        1981       0.10      0.11      0.11       800\n",
      "        1982       0.10      0.11      0.10       800\n",
      "        1983       0.13      0.12      0.12       800\n",
      "        1984       0.10      0.10      0.10       800\n",
      "        1985       0.12      0.12      0.12       800\n",
      "        1986       0.12      0.11      0.12       800\n",
      "        1987       0.12      0.12      0.12       800\n",
      "        1988       0.11      0.10      0.11       800\n",
      "        1989       0.11      0.10      0.11       800\n",
      "\n",
      "    accuracy                           0.11      8000\n",
      "   macro avg       0.11      0.11      0.11      8000\n",
      "weighted avg       0.11      0.11      0.11      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnas = ['acousticness','popularity']\n",
    "X_new = X[columnas]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=.40, random_state=37, stratify=y)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=2)\n",
    "clf.fit(X_train, y_train)    ## Entrenamos con features X_train y clases y_train\n",
    "\n",
    "y_pred = clf.predict(X_test)   ## Predecimos con nuevos datos (los de test X_test)\n",
    "\n",
    "print(\"Accuracy en test set:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred)) ## Evaluamos la predicción comparando y_test con y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab38d5",
   "metadata": {},
   "source": [
    "A continuación veremos los resultados de los parámetros de validación, pero con resultados en promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bc56bd-d23c-4e41-84db-3047bf47c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio Precision: 0.14564017444746372\n",
      "Promedio Recall: 0.14579999999999999\n",
      "Promedio F1-score: 0.14461047992421291\n",
      "Promedio Accucary: 0.14579999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', 'accuracy', 'f1_macro']\n",
    "cv_results = cross_validate(\n",
    "    clf, X, y, cv=10, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('Promedio Precision:', np.mean(cv_results['test_precision_macro']))\n",
    "print('Promedio Recall:', np.mean(cv_results['test_recall_macro']))\n",
    "print('Promedio F1-score:', np.mean(cv_results['test_f1_macro']))\n",
    "print('Promedio Accucary:', np.mean(cv_results['test_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6222f7-ac72-45e0-b769-63f48acd3464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio Precision: 0.10353676356034973\n",
      "Promedio Recall: 0.10195000000000001\n",
      "Promedio F1-score: 0.1012522774557529\n",
      "Promedio Accucary: 0.10194999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', 'accuracy', 'f1_macro']\n",
    "cv_results = cross_validate(\n",
    "    clf, X_new, y, cv=10, scoring=scoring, return_train_score=True)\n",
    "\n",
    "print('Promedio Precision:', np.mean(cv_results['test_precision_macro']))\n",
    "print('Promedio Recall:', np.mean(cv_results['test_recall_macro']))\n",
    "print('Promedio F1-score:', np.mean(cv_results['test_f1_macro']))\n",
    "print('Promedio Accucary:', np.mean(cv_results['test_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9c9f86-389a-49e1-a4c1-f6b6d0916c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def run_classifier(clf, X, y, num_tests=100):\n",
    "    metrics = {'f1-score': [], 'precision': [], 'recall': []}\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        #dividiendo los datos de entrenamiento y validación\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30)\n",
    "        \n",
    "        ### INICIO COMPLETAR ACÁ \n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        #### TIP: en base a los set de entrenamiento, genere la variable 'predictions' \n",
    "        #### que contiene las predicciones del modelo\n",
    "        \n",
    "       \n",
    "        \n",
    "        ### FIN COMPLETAR ACÁ\n",
    "        \n",
    "        metrics['y_pred'] = predictions\n",
    "        metrics['f1-score'].append(f1_score(y_test, predictions, average='micro')) \n",
    "        metrics['recall'].append(recall_score(y_test, predictions, average='micro'))\n",
    "        metrics['precision'].append(precision_score(y_test, predictions, average='micro'))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808859e",
   "metadata": {},
   "source": [
    "A continuación utilizaremos otros clasificador, para ver si podemos obtener mejores resultados con nuestro database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173cf671-7254-4396-a31e-cf9f2e84169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Resultados para clasificador:  Gaussian Naive Bayes\n",
      "Precision promedio: 0.11316166666666665\n",
      "Recall promedio: 0.11316166666666665\n",
      "F1-score promedio: 0.11316166666666665\n",
      "\n",
      "----------------\n",
      "\n",
      "\n",
      "----------------\n",
      "Resultados para clasificador:  KNN\n",
      "Precision promedio: 0.10951666666666666\n",
      "Recall promedio: 0.10951666666666666\n",
      "F1-score promedio: 0.10951666666666665\n",
      "\n",
      "----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier #kNN\n",
    "from sklearn.svm import SVC  # support vector machine\n",
    "\n",
    "c0 = (\"Gaussian Naive Bayes\", GaussianNB())\n",
    "c1 = (\"KNN\", KNeighborsClassifier(n_neighbors=10))\n",
    "#c2 = (\"Support Vector Machines\", SVC())\n",
    "\n",
    "classifiers = [c0, c1,]# c2]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    metrics = run_classifier(clf, X_new, y)   # hay que implementarla en el bloque anterior.\n",
    "    results[name] = metrics\n",
    "    print(\"----------------\")\n",
    "    print(\"Resultados para clasificador: \", name) \n",
    "    print(\"Precision promedio:\", np.array(metrics['precision']).mean())\n",
    "    print(\"Recall promedio:\", np.array(metrics['recall']).mean())\n",
    "    print(\"F1-score promedio:\", np.array(metrics['f1-score']).mean())\n",
    "    print(\"\")\n",
    "    print(\"----------------\\n\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7daeb3c-caa4-4040-8f1a-63cc51ee089f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
